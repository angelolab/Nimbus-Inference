{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nimbus finetuning notebook\n",
    "Nimbus may not work perfectly out of the box for your dataset. This notebook will guide you through the process of finetuning Nimbus for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from nimbus_inference.trainer import Trainer, AugmentationPipeline\n",
    "from nimbus_inference.utils import LmdbDataset, prepare_training_data, MultiplexDataset\n",
    "from nimbus_inference.nimbus import Nimbus, prep_naming_convention\n",
    "from nimbus_inference.viewer_widget import NimbusViewer\n",
    "from ark.utils import example_dataset\n",
    "from alpineer import io_utils\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Set root directory and download example dataset\n",
    "Here we are using the example data located in `/data/example_dataset/input_data`. To modify this notebook to run using your own data, simply change `base_dir` to point to your own sub-directory within the data folder. Set `base_dir`, the path to all of your imaging data (i.e. multiplexed images and segmentation masks). Subdirectory `training_artefacts` will contain all of the data generated by this notebook. In the following, we expect this folder structure, with `fov_1` and `fov_2` either being folders of individual channel images or `.ome.tiff` files that contain all channels in a single file.\n",
    "```bash\n",
    "|-- base_dir\n",
    "|   |-- image_data\n",
    "|   |   |-- fov_1\n",
    "|   |   |-- fov_2\n",
    "|   |-- segmentation\n",
    "|   |   |-- deepcell_output\n",
    "|   |-- training_artefacts\n",
    "|-- ground_truth.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the base directory\n",
    "base_dir = os.path.normpath(\"../data/example_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to test Nimbus finetuning with an example dataset, run the cell below. It will download a dataset consisting of 10 FOVs with 22 channels. You may find more information about the example dataset in the [ark-analysis README](https://github.com/angelolab/ark-analysis/blob/bc6685050dfbef4607874fbbadebd4289251c173/README.md#example-dataset). If you want to use your own data, skip the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset.get_example_dataset(dataset=\"cluster_pixels\", save_dir = base_dir, overwrite_existing = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: set file paths and parameters\n",
    "\n",
    "### All data, images, files, etc. must be placed in the 'data' directory, and referenced via '../data/path_to_your_data'\n",
    "\n",
    "The groundtruth dataframe should be a csv file with the following columns:\n",
    "- `fov`: the field of view name\n",
    "- `cell_id`: the unique identifier for each cell in the fov\n",
    "- `channel`: the channel or marker name\n",
    "- `activity`: the marker activity of the cell (`0`: negative, `1`: positive, `2`: ambiguous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up file paths\n",
    "tiff_dir = os.path.join(base_dir, \"image_data\")\n",
    "deepcell_output_dir = os.path.join(base_dir, \"segmentation\", \"deepcell_output\")\n",
    "groundtruth_path = os.path.join(base_dir, \"noisy_groundtruth.csv\")\n",
    "training_artefacts = os.path.join(base_dir, \"training_artefacts\")\n",
    "nimbus_output_dir = os.path.join(base_dir, \"nimbus_output\")\n",
    "\n",
    "os.makedirs(training_artefacts, exist_ok=True)\n",
    "os.makedirs(nimbus_output_dir, exist_ok=True)\n",
    "\n",
    "# Check if paths exist\n",
    "io_utils.validate_paths([base_dir, tiff_dir, deepcell_output_dir, training_artefacts])\n",
    "\n",
    "# Load the ground truth data\n",
    "groundtruth = pd.read_csv(groundtruth_path)\n",
    "\n",
    "# check if the ground truth data df has column names 'fov', 'cell_id', 'channel' and 'activity'\n",
    "if not set(groundtruth.columns) == set(['fov', 'cell_id', 'channel', 'activity']):\n",
    "    raise ValueError(\n",
    "        \"Ground truth data should have columns 'fov', 'cell_id', 'channel' and 'activity'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Set up input paths and the naming convention for the segmentation data\n",
    "Store names of channels to exclude in the training data in the list below. Either predict all FOVs or specify manually the ones you want to use for Nimbus training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the channels to include\n",
    "include_channels = [\n",
    "    \"CD3\", \"CD4\", \"CD8\", \"CD14\", \"CD20\", \"CD31\", \"CD45\", \"CD68\", \"CD163\", \"CK17\", \"Collagen1\",\n",
    "    \"ECAD\", \"Fibronectin\", \"GLUT1\", \"HLADR\", \"IDO\", \"Ki67\", \"PD1\", \"SMA\", \"Vim\"\n",
    "]\n",
    "\n",
    "# either get all fovs in the folder...\n",
    "fov_names = os.listdir(tiff_dir)\n",
    "# ... or optionally, select a specific set of fovs manually\n",
    "# fovs = [\"fov0\", \"fov1\"]\n",
    "\n",
    "# make sure to filter paths out that don't lead to FoVs, e.g. .DS_Store files.\n",
    "fov_names = [fov_name for fov_name in fov_names if not fov_name.startswith(\".\")] \n",
    "\n",
    "# construct paths for fovs\n",
    "fov_paths = [os.path.join(tiff_dir, fov_name) for fov_name in fov_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the naming convention for the segmentation data in function `segmentation_naming_convention`, that maps the `fov_name` to the path of the associated segmentation output. The below function `prep_deepcell_naming_convention` assumes that all segmentation outputs are stored in one folder, with the `fov_name` as the prefix and `_whole_cell.tiff` as the suffix, as shown below in the visualization of the folder structure. If this does not apply to your data, you have to define a function `segmentation_naming_convention` that takes an element from `fov_paths` and returns a valid path to the segmentation label map you want to use for that fov.\n",
    "\n",
    "```\n",
    "|-- base_dir\n",
    "|   |-- image_data\n",
    "|   |   |-- fov_1\n",
    "|   |   |-- fov_2\n",
    "|   |-- segmentation\n",
    "|   |   |-- deepcell_output\n",
    "|   |   |   |-- fov_1_whole_cell.tiff\n",
    "|   |   |   |-- fov_2_whole_cell.tiff\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare segmentation naming convention that maps a fov_path to the according segmentation label map\n",
    "segmentation_naming_convention = prep_naming_convention(deepcell_output_dir)\n",
    "\n",
    "# test segmentation_naming_convention\n",
    "if os.path.exists(segmentation_naming_convention(fov_paths[0])):\n",
    "    print(\"Segmentation data exists for fov 0 and naming convention is correct\")\n",
    "else:\n",
    "    print(\"Segmentation data does not exist for fov 0 or naming convention is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use the `MultiplexDataset` class to abstract away differences in data representation. The class takes `fov_paths`, `segmentation_naming_convention` and a `suffix` and provides methods `.get_channel(fov, channel)` and `.get_segmentation(fov)` to access the data. The `suffix` is used to filter out files that do not end with the specified suffix. When you use `.ome.tiff` files make sure to set the suffix to `.ome.tiff`, otherwise the ViewerWidget won't be able to display the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultiplexDataset(\n",
    "    fov_paths=fov_paths,\n",
    "    suffix=\".tiff\", # or .png, .jpg, .jpeg, .tif or .ome.tiff\n",
    "    include_channels=include_channels,\n",
    "    segmentation_naming_convention=segmentation_naming_convention,\n",
    "    output_dir=training_artefacts,\n",
    "    magnification=20,\n",
    "    groundtruth_df=groundtruth,\n",
    "    validation_fovs=[\"fov8\", \"fov9\", \"fov10\"], # optional: specify a list of fovs for validation\n",
    ")\n",
    "\n",
    "dataset.prepare_normalization_dict(\n",
    "    quantile=0.999,\n",
    "    n_subset=50,\n",
    "    clip_values=(0, 2),\n",
    "    multiprocessing=False,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Load model and initialize Nimbus application\n",
    "The following code initializes the Nimbus application and loads the model checkpoint. The model was trained on a diverse set of tissues, protein markers, imaging platforms and cell types, still the performance can be improved by fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimbus = Nimbus(\n",
    "    dataset=dataset,\n",
    "    input_shape=[256,256],\n",
    "    model_magnification=10,\n",
    "    device=\"auto\",  # or \"cuda\", \"cpu\", \"mps\" for apple silicon\n",
    "    output_dir=training_artefacts,\n",
    ")\n",
    "\n",
    "# check if all inputs are valid\n",
    "nimbus.check_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Prepare training data\n",
    "The following code will preprocess the training data and prepare it for training. It will create a folder `training_artefacts` in the `base_dir` and store the training data in the subfolder `training_artefacts/training_data` and the validation data in subfolder `training_artefacts/validation_data` as .lmdb files. The training data consists of the images and segmentation masks of the FOVs specified in `fovs_to_train` as LMDB files. The training data is split into training and validation data with a ratio of 90:10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_training_data(\n",
    "        nimbus=nimbus,\n",
    "        dataset=dataset,\n",
    "        output_dir=training_artefacts,\n",
    "        tile_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Setup the training pipeline and start training\n",
    "The following code will setup the training pipeline, load the data and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training data\n",
    "training_data = LmdbDataset(os.path.join(training_artefacts, \"training\"))\n",
    "validation_data = LmdbDataset(os.path.join(training_artefacts, \"validation\"))\n",
    "\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    nimbus=nimbus,\n",
    "    train_dataset=training_data,\n",
    "    validation_dataset=validation_data,\n",
    "    checkpoint_name=\"nimbus_example_data_finetuning.pt\",\n",
    "    batch_size=8,\n",
    "    learning_rate=1e-4,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Load new model checkpoint\n",
    "The following code will initialize the nimbus application and load the new model checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimbus = Nimbus(\n",
    "    dataset=dataset,\n",
    "    input_shape=[512,512],\n",
    "    model_magnification=10,\n",
    "    device=\"auto\",\n",
    "    output_dir=nimbus_output_dir,\n",
    "    checkpoint=\"nimbus_example_data_finetuning.pt\",\n",
    "    save_predictions=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Make predictions with the model\n",
    "Nimbus will iterate through your samples and store predictions and a file named `nimbus_cell_table.csv` that contains the mean-per-cell predicted marker confidence scores in the sub-directory called `nimbus_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_table = nimbus.predict_fovs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: View multiplexed channels and Nimbus predictions side-by-side\n",
    "Select an FOV and one marker image per channel to inspect the imaging data and associated Nimbus predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = NimbusViewer(dataset=dataset, output_dir=nimbus_output_dir)\n",
    "viewer.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Nimbus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
